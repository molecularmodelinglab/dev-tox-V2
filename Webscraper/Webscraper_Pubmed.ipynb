{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WebScraper for PubMed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The following Code was run on GoogleCollab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping results for keyword (1): captopril pregnancy trimester risk\n",
      "Scraping results for keyword (2): Naloxone pregnancy trimester risk\n",
      "Scraping results for keyword (3): Mifepristone pregnancy trimester risk\n",
      "Scraped data saved to ./Scrapper_results/Scraped_Articles_Trimester.xlsx\n",
      "Curated data saved to ./Scrapper_results/Curated_Compounds_Trimester.xlsx\n"
     ]
    }
   ],
   "source": [
    "# RUN THIS CELL FIRST\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import os\n",
    "\n",
    "def scrape_pubmed(keyword, num_results, results_df):\n",
    "    base_url = \"https://pubmed.ncbi.nlm.nih.gov\"\n",
    "    search_url = f\"{base_url}/?term={keyword}\"\n",
    "\n",
    "    response = requests.get(search_url)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    articles = soup.find_all(\"article\", class_=\"full-docsum\")\n",
    "    articles = articles[:num_results]\n",
    "\n",
    "    rows = []  # List to store article data\n",
    "\n",
    "    for article in articles:\n",
    "        title_elem = article.find(\"a\", class_=\"docsum-title\")\n",
    "        title = title_elem.text.strip() if title_elem else \"Title not found\"\n",
    "\n",
    "        authors_elem = article.find(\"span\", class_=\"docsum-authors\")\n",
    "        authors = authors_elem.text.strip() if authors_elem else \"Authors not found\"\n",
    "\n",
    "        journal_elem = article.find(\"span\", class_=\"docsum-journal-citation\")\n",
    "        journal = journal_elem.text.strip() if journal_elem else \"Journal not found\"\n",
    "\n",
    "        abstract_url_elem = article.find(\"a\", class_=\"docsum-title\")\n",
    "        if abstract_url_elem:\n",
    "            abstract_url = base_url + abstract_url_elem.get(\"href\")\n",
    "            abstract_response = requests.get(abstract_url)\n",
    "            abstract_response.raise_for_status()\n",
    "            abstract_soup = BeautifulSoup(abstract_response.content, 'html.parser')\n",
    "            abstract_text_elem = abstract_soup.find(\"div\", class_=\"abstract-content\")\n",
    "            abstract_text = abstract_text_elem.text.strip() if abstract_text_elem else \"Abstract not found\"\n",
    "        else:\n",
    "            abstract_text = \"Abstract URL not found\"\n",
    "\n",
    "        rows.append({\n",
    "            'Keyword': keyword,\n",
    "            'Title': title,\n",
    "            'Authors': authors,\n",
    "            'Journal': journal,\n",
    "            'Abstract': abstract_text\n",
    "        })\n",
    "\n",
    "    new_df = pd.concat([results_df, pd.DataFrame(rows)], ignore_index=True)\n",
    "    return new_df\n",
    "\n",
    "def curate_compounds(df):\n",
    "    curated_rows = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        abstract = row['Abstract']\n",
    "\n",
    "        if (re.search(r'\\b(human|humans|participants|females|mothers|babies|children|women|deliveries|infants|review)\\b', abstract, re.IGNORECASE) and\n",
    "            re.search(r'\\b\\d{3,}\\b', abstract)):\n",
    "            curated_rows.append(row)\n",
    "\n",
    "        elif (re.search(r'\\b(rat|rats|rabbit|rabbits|mouse|mice)\\b', abstract, re.IGNORECASE) and\n",
    "              re.search(r'\\b\\d+\\s*(mg|g|µg)\\b', abstract, re.IGNORECASE)):\n",
    "            curated_rows.append(row)\n",
    "\n",
    "        elif (re.search(r'\\b(animal|animals|mouse|mice|rat|rabbit|rabbits)\\b', abstract, re.IGNORECASE) and\n",
    "              re.search(r'\\b\\d+\\s*(mg|g|µg)\\b', abstract, re.IGNORECASE)):\n",
    "            curated_rows.append(row)\n",
    "\n",
    "    curated_df = pd.DataFrame(curated_rows).reset_index(drop=True)\n",
    "    return curated_df\n",
    "\n",
    "# Main script\n",
    "def main():\n",
    "    # Update this path to match your local Excel file location\n",
    "    input_file = 'keywords_example.xlsx'  # e.g., 'C:/Users/YourUser/Documents/keywords_trimester.xlsx'\n",
    "    \n",
    "    # Load input data\n",
    "    df = pd.read_excel(input_file)\n",
    "\n",
    "    num_results = 3\n",
    "    results_df = pd.DataFrame(columns=['Keyword', 'Title', 'Authors', 'Journal', 'Abstract'])\n",
    "\n",
    "    for count, keyword in enumerate(df['Compound_name'], start=1):\n",
    "        print(f\"Scraping results for keyword ({count}): {keyword}\")\n",
    "        results_df = scrape_pubmed(keyword, num_results, results_df)\n",
    "\n",
    "        if count >= 600:\n",
    "            print(\"Processed 600 compounds. Stopping the scraping process.\")\n",
    "            break\n",
    "\n",
    "    # Ensure output directory exists\n",
    "    output_dir = './Scrapper_results'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Save scraped results\n",
    "    output_file = os.path.join(output_dir, 'Scraped_Articles_Trimester.xlsx')\n",
    "    results_df.to_excel(output_file, index=False)\n",
    "    print(f\"Scraped data saved to {output_file}\")\n",
    "\n",
    "    # Curate and save curated results\n",
    "    curated_df = curate_compounds(results_df)\n",
    "    curated_output_file = os.path.join(output_dir, 'Curated_Compounds_Trimester.xlsx')\n",
    "    curated_df.to_excel(curated_output_file, index=False)\n",
    "    print(f\"Curated data saved to {curated_output_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rdkit_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
